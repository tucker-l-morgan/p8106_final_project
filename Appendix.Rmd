---
title: "Appendix"
author: 'Zachary Katz (UNI: zak2132)'
date: "5/2/2022"
output: pdf_document
---

```{r imputing w mice}
# Impute all missing values using MICE
library(mice)

# 50 iterations using predictive mean matching
# TM: changed object name because output is not a dataframe
# TM: I also think this method is most common for inferential settings where inference is performed with each of the 5 imputed datasets, but I don't think we care to do this; see: https://bookdown.org/max/FES/imputation-methods.html
# the above resource also recommends imputing during the resampling process so that imputation variability is captured, but I don't think Yifei mentioned anything about this? So I think we go with bagging imputation at the very start
imputed_mids = mice(all_df, 
                  m = 5,
                  maxit = 50,
                  method = "pmm",
                  seed = 100)

summary(imputed_mids)
# this function outputs the completed dataset from the mids object
completed_df <- complete(imputed_mids)
skimr::skim(completed_df)
```

```{r imputing w Hmisc}
# Impute using Hmisc
# Note: assumes linearity in the variables being predicted
library(Hmisc)

# Example of imputing glucose column using median value
# TM: the weakest option IMO
all_df$glucose = with(all_df, impute(glucose, median))
```

```{r correlation plots, include = FALSE}
# We should consider imputing for glucose only given highest correlation of these 4 missing variables with ten_year_chd, as well as much higher missingness rate
# Beyond that, drop other missing rows

# Correlation plot for all predictors
all_preds_matrix = model.matrix(ten_year_chd ~ ., data = cleaned_df)[, -1]

all_vars_corrs = corrplot(cor(all_preds_matrix), method = "circle", type = "full")

all_vars_corrs

# Alternative using ggcorrplot for all vars
ggplot_corr_alt = model.matrix(~0 + ., data = cleaned_df) %>%
  cor(use = "pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type = "lower", lab = TRUE, lab_size = 2)

ggplot_corr_alt
```

```{r imputing w preProcess using diabetes}
# Bagged tree imputation using preProcess (note: could also use 'knnImpute' for method)
# Use diabetes, since glucose is most highly correlated with it, and there is a clearly explainable reason (glucose's role in diabetes)
bag_impute = cleaned_df %>% 
  dplyr::select(glucose, diabetes) %>% 
  preProcess(method = "bagImpute")

imputed_glucose = predict(bag_impute, 
                          cleaned_df %>% dplyr::select(glucose, diabetes))

cleaned_df$glucose = imputed_glucose$glucose

# Check missingness post-imputation for glucose
# Still missing 251 rows; drop them
rows_missing_data_post_impute = sum(!complete.cases(cleaned_df))

final_df = cleaned_df %>% 
  na.omit()

# Open questions:
# Should we impute for both training and testing data, or just training data?
# Should we impute glucose based on diabetes data only, or also other vars?
```

```{r random forest}
# Random forest
set.seed(2022)

# RF grid
rf_grid = expand.grid(mtry = 1:8,
                       splitrule = "gini",
                       min.node.size = seq(from = 2, to = 10, by = 2))

# Train random forest model
rf_fit = train(x = x_train,
              y = y_train,
              method = "ranger",
              tuneGrid = rf_grid,
              metric = "ROC",
              trControl = ctrl,
              preProcess = c("center", "scale", "BoxCox"))

ggplot(rf_fit, highlight = TRUE)

# Performance on test data

# Class probabilities
rf_pred_test_probs = predict(rf_fit, newdata = x_test, type = "prob")[,1]

rf_pred_test_class = predict(rf_fit, newdata = x_test, type = "raw")

# Confusion matrix
# Performs identically to NIR on test data :(
confusionMatrix(data = rf_pred_test_class,
                reference = y_test)
```

```{r boosting}
# Boosting
set.seed(2022)

# Grid search for adaboost
adaboost_grid = expand.grid(n.trees = c(2000,3000,4000,5000),
                         interaction.depth = 1:6,
                         shrinkage = c(0.0005,0.001,0.002),
                         n.minobsinnode = 1)

# Train boosting model
boost_fit = train(x = x_train,
                  y = y_train, 
                  tuneGrid = adaboost_grid,
                  trControl = ctrl,
                  method = "gbm",
                  distribution = "adaboost",
                  metric = "ROC",
                  verbose = FALSE,
                  preProcess = c("center", "scale", "BoxCox"))

ggplot(boost_fit, highlight = TRUE)

# Performance on test data

# Class probabilities
boost_pred_test_probs = predict(boost_fit, newdata = x_test, type = "prob")[,1]

boost_pred_test_class = predict(boost_fit, newdata = x_test, type = "raw")

# Confusion matrix
# Performs 0.001 better than NIR (p-value Acc > NIR of 0.485, so not statistically significant different from NIR on test data)
confusionMatrix(data = boost_pred_test_class,
                reference = y_test)
```

```{r random forest without imputation}
# Random forest without imputation
set.seed(2022)

# RF grid
rf_grid = expand.grid(mtry = 1:8,
                      splitrule = "gini",
                      min.node.size = seq(from = 2, to = 10, by = 2))

# Train random forest model
rf_fit_without_impute = train(ten_year_chd ~ .,
                              data = training_df_dropped,
                              method = "ranger",
                              tuneGrid = rf_grid,
                              metric = "ROC",
                              trControl = ctrl,
                              preProcess = c("center", "scale", "BoxCox"))
```

```{r boosting without imputation}
# Boosting without imputation
set.seed(2022)

# Grid search for adaboost
adaboost_grid = expand.grid(n.trees = c(2000,3000,4000,5000),
                         interaction.depth = 1:6,
                         shrinkage = c(0.0005,0.001,0.002),
                         n.minobsinnode = 1)

# Train boosting model
boost_fit_without_impute = train(ten_year_chd ~ .,
                                data = training_df_dropped, 
                                tuneGrid = adaboost_grid,
                                trControl = ctrl,
                                method = "gbm",
                                distribution = "adaboost",
                                metric = "ROC",
                                verbose = FALSE,
                                preProcess = c("center", "scale", "BoxCox"))
```

```{r RF with bagged impute}
set.seed(2022)

# Just for fun: let's try random forest with bagged imputation
preprocess_recipe_bagged = recipe(ten_year_chd ~ ., data = training_df) %>%
  step_impute_bag(all_predictors()) %>%  # Bagged imputation 
  step_BoxCox(all_numeric_predictors()) %>% # transform predictors
  step_center(all_numeric_predictors()) %>% # center and scale numeric predictors
  step_scale(all_numeric_predictors())

rf_fit_bagged = train(preprocess_recipe_bagged,
              data = training_df,
              method = "ranger",
              tuneGrid = rf_grid,
              metric = "ROC",
              trControl = ctrl)
```