---
title: 'Applications in Machine Learning to Predict Coronary Heart Disease'
author: 'Zachary Katz (zak2132), Hun Lee (sl4836), and Tucker Morgan (tlm2152)'
date: "5/12/2022"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 3
  fontsize: 11pt
  geometry: margin=0.75in
  header-includes:
    -\usepackage{fancyhdr}
    -\usepackage{lipsum}
    -\pagestyle{fancy}
    -\fancyhead[R]{\thepage}
    -\fancypagestyle{plain}{\pagestyle{fancy}}
---

```{r libraries and setup, include = FALSE, echo = FALSE}
# TM: are we using all of these? can we remove if not?
library(tidyverse)
library(ggplot2)
library(skimr)
library(visdat)
library(reshape2)
library(DataExplorer)
library(ggcorrplot) # I prefer this option, we should remove whichever lib we don't use
library(caret)
library(vip)
library(mgcv)
library(klaR)
library(pROC)
library(ggridges)
library(corrplot)
library(AppliedPredictiveModeling)
library(patchwork)
library(Hmisc)
library(groupdata2)
library(reshape)
library(recipes)
library(glmnet)
library(rpart.plot)
library(MASS)
library(glmnet)
library(nnet)
library(xgboost)
library(NeuralNetTools)  
library(Ckmeans.1d.dp)
library(kableExtra)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE,
  include = FALSE,
  fig.height = 6,
  fig.width = 8
  )

theme_set(theme_minimal() + theme(
  legend.position = "bottom",
  plot.title = element_text(hjust = 0.5)
))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis",
  digits = 3)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Introduction

```{r importing and cleaning data, include = FALSE, eval = TRUE}
# Read in all data
# source: https://raw.githubusercontent.com/TarekDib03/Analytics/master/Week3%20-%20Logistic%20Regression/Data/framingham.csv
all_df = read_csv("FHS.csv")

# Factor labels for categorical variables and other recoding
cleaned_df = all_df %>% 
  mutate(male = factor(male),
         current_smoker = factor(current_smoker),
         bp_meds = factor(bp_meds),
         prevalent_stroke = factor(prevalent_stroke),
         prevalent_hyp = factor(prevalent_hyp),
         diabetes = factor(diabetes),
         ten_year_chd = factor(ten_year_chd))  %>%
  mutate(ten_year_chd = ifelse(ten_year_chd == "1", "CHD_present","CHD_absent") %>%
           fct_relevel("CHD_present", "CHD_absent")) %>%
  dplyr::rename(sex = male) %>%
  mutate(sex = ifelse(sex == "1", "male","female") %>%
           fct_relevel("male", "female")) %>% 
  mutate(
    education = case_when(
      education == "1" ~ "some_HS",
      education == "2" ~ "HS_grad",
      education == "3" ~ "some_college",
      education == "4" ~ "college_grad"
    ),
    current_smoker = recode(
      current_smoker,
      "1" = "yes",
      "0" = "no"
    ),
    bp_meds = recode(
      bp_meds,
      "1" = "yes",
      "0" = "no"
    ),
    prevalent_stroke = recode(
      prevalent_stroke,
      "1" = "yes",
      "0" = "no"
    ),
    prevalent_hyp = recode(
      prevalent_hyp,
      "1" = "yes",
      "0" = "no"
    ),
    diabetes = recode(
      diabetes,
      "1" = "yes",
      "0" = "no"
    ),
    education = factor(education, levels = c("some_HS", "HS_grad", "some_college", "college_grad"))
  )
```

Heart disease accounts for roughly 695,000 fatalities annually in the United States alone, with known risk factors that include high cholesterol, smoking, and blood pressure. In this project, we aim to utilize observations from the Framingham cohort study to predict whether a particular study subject will or will not develop coronary heart disease in the next decade. Our data subset of longitudinal observations come from Kaggle; its cleaning generally entailed converting appropriate variables to factors (and re-leveling where needed), renaming and recoding binary 1/0 variables with more descriptive "yes" and "no" for ease of interpretation, and excluding observations with missing data on any measure. Prior to such exclusions, the full set of data contained 4,240 total observations across 16 variables, which are:

$\text{\underline{7 categorical predictors}}$: `sex` (self-reported subject sex that takes values "male" or "female"); `education` (study participant's education that takes ordinal, mutually exclusive values "some_HS" (some high school completed), "HS_grad" (completed high school but did not attend college), "some_college" (attended college but did not graduate), and "college_grad" (graduated university)); `current_smoker` (binary "yes" or "no" indicating whether the participant was a smoker at the time of physical examination); `bp_meds` (binary "yes" or "no" indicating whether the participant was using anti-hypertensive medications at the time of physical examination); `prevalent_stroke` (binary "yes" or "no" indicating whether the participant had experienced stroke by the time of physical examination); `prevalent_hyp` (binary "yes" or "no" indicating whether the participant was being treated for active hypertension at the time of physical examination); and `diabetes` (binary "yes" or "no" indicating whether the participant was diagnosed as diabetic according to pre-specified criteria at the time of physical examination).

$\text{\underline{8 continuous numeric predictors}}$: `age` (age in years at the time of medical examination); `cigs_per_day` (average number of cigarettes smoked each day at the time of medical examination, notably not conditioned on smoking status (i.e. for those with `current_smoker` status as "no", should take the value 0)); `tot_chol` (total blood cholesterol in mg/dL at the time of physical examination); `sys_BP` (systolic blood pressure in mm Hg at the time of physical examination); `dia_BP` (diastolic blood pressure in mm Hg at the time of physical examination); `bmi` (body mass index in $kg/m^2$ in mm Hg at the time of physical examination); `heart_rate` (resting heart rate in beats per minute at the time of physical examination); and `glucose` (blood glucose level in mg/dL at the time of physical examination). 

Finally, beyond our 15 covariates lies our outcome (response) variable `ten_year_chd`, which is a binary indicator for the presence or absence of coronary heart disease (CHD) at 10 years of follow-up. Of our 4,240 observations, 3,596 (84.8%) have absence of CHD, whereas 644 (15.2%) have CHD present -- a notable class imbalance.

## EDA Figures

```{r missing data}
# Number of rows with any missing data
rows_missing_data = sum(!complete.cases(cleaned_df))

# Missing data pattern
# 582 rows (13.7% of observations) missing 1+ data points
# Highest missingness rates: glucose (9.15%), education (2.48%), bp_meds (1.25%), tot_chol (1.18%)
missing_data_viz = cleaned_df %>% 
  vis_miss()
```

Notably, 582 rows (13.7% of our observations) lacked at least one data point, with `glucose` accounting for the plurality of missing data (9.15% missing rate). Moving forward, we assume that our data is missing at random, and consequently build a KNN imputation step (using five nearest neighbors) into our preprocessing functionality, which also includes centering, scaling, and BoxCox transformations where possible. 

```{r continuous variable feature plot, eval = TRUE}
# Distributions of continuous variables factored by outcome
continuous_vars_df = cleaned_df %>% 
  dplyr::select(age, cigs_per_day, tot_chol, sys_bp, dia_bp, bmi, heart_rate, glucose, ten_year_chd) %>% 
  as.data.frame()

continuous_explore = continuous_vars_df %>% 
  melt(id.vars = "ten_year_chd") %>% 
  ggplot(aes(x = value, y = ten_year_chd)) + 
  stat_density_ridges(aes(color = ten_year_chd, fill = ten_year_chd), alpha = 0.2, quantile_lines = TRUE, quantiles = 2, jittered_points = TRUE) + 
  facet_wrap(~variable, scales = "free", nrow = 2) + 
  labs(x = "",
       y = "Density",
       fill = "10 Year CHD Status",
       color = "10 Year CHD Status") +
  theme(legend.position = "none")
```

```{r categorical variable viz, eval = TRUE}
# Distributions of categorical variables factored by outcome
categorical_vars_df = cleaned_df %>% 
  dplyr::select(-age, -cigs_per_day, -tot_chol, -sys_bp, -dia_bp, -bmi, -heart_rate, -glucose) %>% 
  as.data.frame()

categorical_explore = categorical_vars_df %>% 
  melt(id.vars = "ten_year_chd") %>% 
  ggplot(aes(x = value, fill = ten_year_chd)) + 
  geom_bar(position = "fill") + 
  facet_wrap(~variable, scales = "free", nrow = 2) + 
  scale_y_continuous(labels = scales::percent) + 
  labs(x = "",
        y = "Proportion by CHD Status",
       fill = "10 Year CHD Status",
       color = "10 Year CHD Status")
```

```{r combined eda figure, eval = TRUE}
continuous_explore / categorical_explore + plot_annotation(
  title = "Fig.1: Distributions of Predictors By Outcome Class"
)
```

When we stratify the distributions of our continuous predictors by outcome status, we find the most substantial differences in median age and systolic blood pressure for those with and without CHD at 10 years. Looking at the proportions that have CHD present or absent across levels of our factor variables, there appears to be the most substantial differences for stroke history, diabetes status, and blood pressure medication status.

```{r correlation plot, eval = TRUE}
model.matrix(~0 + ., data = cleaned_df) %>%
  cor(use = "pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type = "lower", lab = TRUE, lab_size = 2, title = "Fig.2: Correlation of Predictors")
```

We observe no major multicollinearities, with the highest correlations (all sub-0.80) found between systolic and diastolic blood pressure, cigarette smoking and cigarettes smoked per day, prevalent hypertension and blood pressure, and glucose levels and diabetes comorbidity. In addition, CHD status is most correlated with age, systolic blood pressure, and prevalence of hypertension, which is unsurprising given our prior exploratory visualizations stratified by CHD class.

## Modeling

In prior work, we attempted to rectify class imbalance through upsampling and downsampling. However, because this method was ineffective in improving our AUC, the focus here was on trying additional models, including ones with increased flexibility. We began by splitting the data into 80% training set and 20% test set. Given lack of major collinearity and KNN imputation for missing data points, all available predictors were included in each model unless a specific model selected them out during the training process. In total, [TO DO] kinds of models were used, with a pre-processing step (imputation, centering, scaling, and Box-Cox transformations) included in each iteration of model training under 10-fold cross-validation, repeated five times. Seeds were set in each instance to ensure reproducibility of the data, given the many assumptions and tuning parameters in our variety of models. All tuning parameters were selected using cross-validation in model training to find the optimal model that maximized AUC. Our models were:

* **Penalized logistic regression** (elastic net, binomial family, logit link), with objective function that includes loss and penalty given our high number of predictors, and tuning on $\alpha$ (mixing proportion) and $\lambda$ (total penalization) for our final model

* **Generalized additive model (GAM)** (also binomial family, logit link), with potential inclusion of flexible nonlinearities for some predictors, performed in both `mgcv` for more flexibility and in `caret` for model comparison, and using general cross-validation to determine the smoothness of each operator $\hat{f_j}$ and to search over `GCV.Cp` given  unknown scale parameter

* **Multivariate adaptive regression splines (MARS)** using `earth`, with tuning parameters (1) degree of features (number of possible hinge functions per parameter) and (2) number of terms (may not equal the number of predictors given the possibility of multiple hinge functions), including a stepwise model building procedure involving the addition of piecewise linear models / spline bases, followed by a pruning procedure

* **Linear discriminant analysis (LDA)**, which has no tuning parameters and assumes normally distributed features to classify by nearest centroid following data sphering and projection onto smaller subspaces, tends to work reasonably well with small n or well-separated classes, which doesn't appear true in our case and may make the model less robust

* **Random Forest**, [TO DO]

* **Boosting**, [TO DO]

* **Trees (CIT and CART)**, [TO DO]

* **Support Vector Machine (Linear and Radial Kernels)**, [TO DO -- make sure to mention something about probabilities/ROC being poor metrics for SVM]

* **Neural Network**, [TO DO]

```{r model tuning setup full data set}
set.seed(2022)

# Training/testing partition
index_train = createDataPartition(cleaned_df$ten_year_chd, 
                                  p = 0.8,
                                  list = FALSE)

training_df = cleaned_df[index_train, ]
testing_df = cleaned_df[-index_train, ]

# Model matrices
x_train = model.matrix(ten_year_chd ~ ., training_df)[, -1] # Note that if a row has NAs, it is by default removed using model.matrix!
x_test = model.matrix(ten_year_chd ~ ., testing_df)[, -1]
y_train = training_df$ten_year_chd
y_test = testing_df$ten_year_chd

# Train control with 10-fold cross-validation repeated 5 times
ctrl = trainControl(method = "repeatedcv",
                    repeats = 5,
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE)
```

```{r preprocessing with recipe}
# Preprocessing and feature engineering with recipe (including imputation)
# Note: assuming data is MAR

# recipe of preprocessing steps
preprocess_recipe = recipe(ten_year_chd ~ ., data = training_df) %>%
  step_impute_knn(all_predictors(), neighbors = 5) %>%  # KNN imputation based on 5 nearest neighbors
  step_BoxCox(all_numeric_predictors()) %>% # transform predictors
  step_center(all_numeric_predictors()) %>% # center and scale numeric predictors
  step_scale(all_numeric_predictors())
```

```{r GLMNet imputation in caret}
# Penalized logistic regression with imputation in caret function directly (NOT recipes)
set.seed(2022)

glm_grid = expand.grid(alpha = seq(0, 1, length = 11),
                       lambda = exp(seq(-8, -3, length = 19)))

ctrl_glmnet = trainControl(method = "repeatedcv",
                    repeats = 5,
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE,
                    preProcOptions = list(k = 5))

logit_next = train(ten_year_chd ~ .,
                   data = training_df,
                   na.action = na.pass,
                  method = "glmnet",
                  tuneGrid = glm_grid,
                  metric = "ROC",
                  trControl = ctrl_glmnet,
                  family = "binomial",
                  preProcess = c("knnImpute", "center", "scale", "BoxCox"))

# Test data: predicted probabilities
glmnet_pred_test_probs = predict(logit_next, newdata = testing_df, type = "prob",
                                 na.action = na.pass)[,1]

# Test data: predicted classes
glmnet_pred_test_class = predict(logit_next, newdata = testing_df, type = "raw",
                                 na.action = na.pass)

# Test data: confusion matrix
# Accuracy: 0.854
confusionMatrix(data = glmnet_pred_test_class,
                reference = y_test)
```

```{r random forest imputation with recipes}
# Random forest with imputation from recipes package
set.seed(2022)

# RF grid
rf_grid_one = expand.grid(mtry = 1:8,
                      splitrule = "gini",
                      min.node.size = seq(from = 6, to = 14, by = 2))

# Train random forest model
rf_fit = train(preprocess_recipe,
              data = training_df,
              method = "ranger",
              tuneGrid = rf_grid_one,
              metric = "ROC",
              trControl = ctrl)

# Test data: predicted probabilities
rf_pred_test_probs = predict(rf_fit, newdata = testing_df, type = "prob")[,1]

# Test data: predicted classes
rf_pred_test_class = predict(rf_fit, newdata = testing_df, type = "raw")

# Test data: confusion matrix
# Accuracy: 0.849
confusionMatrix(data = rf_pred_test_class,
                reference = y_test)
```

```{r random forest imputation in caret}
# Random forest with imputation from caret function directly (NOT recipes)
set.seed(2022)

ctrl_RF = trainControl(method = "repeatedcv",
                    repeats = 5,
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE,
                    preProcOptions = list(k = 5))

rf_grid_two = expand.grid(mtry = 1:8,
                      splitrule = "gini",
                      min.node.size = seq(from = 2, to = 10, by = 2))

rf_caret = train(ten_year_chd ~ .,
                   data = training_df,
                   na.action = na.pass,
                  method = "ranger",
                  tuneGrid = rf_grid_two,
                  metric = "ROC",
                  trControl = ctrl_RF,
                  preProcess = c("knnImpute", "center", "scale", "BoxCox"))

# Test data: predicted probabilities
rf_caret_pred_test_probs = predict(rf_caret, newdata = testing_df, type = "prob",
                                   na.action = na.pass)[,1]

# Test data: predicted classes
rf_caret_pred_test_class = predict(rf_caret, newdata = testing_df, type = "raw",
                                   na.action = na.pass)

# Test data: confusion matrix
# Accuracy: 0.849
confusionMatrix(data = rf_caret_pred_test_class,
                reference = y_test)
```

```{r boosting imputation with recipes}
# Boosting with imputation from recipes package
set.seed(2022)

# Grid search for adaboost
adaboost_grid = expand.grid(n.trees = c(4000,5000,6000),
                            interaction.depth = 1:6,
                            shrinkage = c(0.001,0.002,0.004),
                            n.minobsinnode = 1)

# Train boosting model
boost_fit = train(preprocess_recipe,
                  data = training_df,
                  tuneGrid = adaboost_grid,
                  trControl = ctrl,
                  method = "gbm",
                  distribution = "adaboost",
                  metric = "ROC",
                  verbose = FALSE)

# Test data: predicted probabilities
boost_pred_test_probs = predict(boost_fit, newdata = testing_df, type = "prob")[,1]

# Test data: predicted classes
boost_pred_test_class = predict(boost_fit, newdata = testing_df, type = "raw")

# Test data: confusion matrix
# Accuracy: 0.854
confusionMatrix(data = boost_pred_test_class,
                reference = y_test)
```

```{r boosting imputation with caret}
# Boosting with imputation directly from caret (NOT recipes)
set.seed(2022)

ctrl_boost = trainControl(method = "repeatedcv",
                    repeats = 5,
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE,
                    preProcOptions = list(k = 5))

# Train boosting model
boost_caret = train(ten_year_chd ~ .,
                   data = training_df,
                   na.action = na.pass,
                  tuneGrid = adaboost_grid,
                  trControl = ctrl_boost,
                  method = "gbm",
                  distribution = "adaboost",
                  metric = "ROC",
                  verbose = FALSE,
                  preProcess = c("knnImpute", "center", "scale", "BoxCox"))

# Test data: predicted probabilities
boost_caret_pred_test_probs = predict(boost_caret, newdata = testing_df, type = "prob",
                                   na.action = na.pass)[,1]

# Test data: predicted classes
boost_caret_pred_test_class = predict(boost_caret, newdata = testing_df, type = "raw",
                                   na.action = na.pass)

# Test data: confusion matrix
# Accuracy: 0.851
confusionMatrix(data = boost_caret_pred_test_class,
                reference = y_test)
```

```{r CART tree}
# CART tree with recipe imputation
set.seed(2022)

cart_fit = train(preprocess_recipe,
                  data = training_df, 
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-9,-5, len = 50))),
                   trControl = ctrl,
                   metric = "ROC")

# Test data: predicted probabilities
cart_pred_prob_test = predict(cart_fit, newdata = testing_df,
                       type = "prob")[,1]

# Test data: predicted classes
cart_pred_class_test = predict(cart_fit, newdata = testing_df, type = "raw")

# Test data: confusion matrix
# Accuracy: 0.799
confusionMatrix(data = cart_pred_class_test,
                reference = y_test)
```

```{r CIT tree}
# CIT tree with recipe imputation
set.seed(2022)

cit_fit = train(preprocess_recipe,
                  data = training_df,
                   method = "ctree",
                   tuneGrid = data.frame(mincriterion = 1-exp(seq(-3, 0, length = 50))),
                   metric = "ROC",
                   trControl = ctrl)

# Test data: predicted probabilities
cit_pred_prob_test = predict(cit_fit, newdata = testing_df,
                       type = "prob")[,1]

# Test data: predicted classes
cit_pred_class_test = predict(cit_fit, newdata = testing_df, type = "raw")

# Test data: confusion matrix
# Accuracy: 0.844
confusionMatrix(data = cit_pred_class_test,
                reference = y_test)
```

```{r SVM linear caret impute}
# SVM linear with impute in caret directly
# ROC seems < 0.6, quite poor
set.seed(2022)

ctrl_linear_svm = trainControl(method = "repeatedcv",
                    repeats = 5,
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE,
                    preProcOptions = list(k = 5))

# I know we're told not to do this, but including Platt's probabilistic outputs here just to see...
svm_linear_fit = train(ten_year_chd ~ .,
                       data = training_df,
                       na.action = na.pass,
                       method = "svmLinear",
                       tuneGrid = data.frame(C = exp(seq(-5, 2, len = 50))),
                       trControl = ctrl_linear_svm,
                       prob.model = TRUE,
                  preProcess = c("knnImpute", "center", "scale", "BoxCox")
                       )

# Test data: predicted probabilities
svm_linear_pred_prob_test = predict(svm_linear_fit, newdata = testing_df,
                       type = "prob", na.action = na.pass)[,1]

# Test data: predicted classes
svm_linear_pred_class_test = predict(svm_linear_fit, newdata = testing_df, type = "raw",
                                     na.action = na.pass)

# Test data: confusion matrix
# Accuracy: 0.849
confusionMatrix(data = svm_linear_pred_class_test,
                reference = y_test)
```

```{r loading svm radial results}
load("svm_radial_res.RData")
```

```{r SVM radial caret impute, eval = FALSE}
# Train control with 10-fold cross-validation repeated 5 times
ctrl_radial_svm = trainControl(method = "repeatedcv",
                               repeats = 5,
                               summaryFunction = twoClassSummary,
                               classProbs = TRUE,
                               preProcOptions = list(k = 5))

# down-sampling for radial svm
set.seed(2022)
down_ind <- createDataPartition(training_df$ten_year_chd,
                                p = 0.15,
                                list = FALSE)
down_df <- training_df[down_ind,]

# Trying SVM with radial classifier for fun
set.seed(2022)

svm_grid = expand.grid(C = exp(seq(-2, 3, len = 50)),
                       sigma = exp(seq(-3, 0, len = 50)))

# I know we're told not to do this, but including Platt's probabilistic outputs here just to see...
svm_radial_fit_impute = train(ten_year_chd ~ .,
                             data = down_df,
                             method = "svmRadialSigma",
                             tuneGrid = svm_grid,
                             trControl = ctrl_radial_svm,
                             prob.model = TRUE,
                             na.action = na.pass,
                             preProcess = c("knnImpute", "center", "scale", "BoxCox"))

svm_radial_pred_prob_test = predict(svm_fit_impute_probs, newdata = testing_df,
                       type = "prob", na.action = na.pass)[,1]

svm_radial_pred_class_test = predict(svm_fit_impute_probs, newdata = testing_df, type = "raw",
                                     na.action = na.pass)
```

```{r LDA}
# LDA in caret, imputation from caret (recipes gives errors, non-numeric argument to binary operator)
set.seed(2022)

lda_ctrl = trainControl(method = "repeatedcv",
                    repeats = 5,
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE,
                    preProcOptions = list(k = 5))

LDA_model_caret = train(ten_year_chd ~ .,
                  data = training_df,
                  na.action = na.pass,
                  method = "lda",
                  metric = "ROC",
                  trControl = lda_ctrl,
                  preProcess = c("knnImpute", "center", "scale", "BoxCox"))

# Examine results
# AUC: 0.716
LDA_model_caret$results

# Test data: predicted probabilities
lda_pred_prob_test = predict(LDA_model_caret, newdata = testing_df,
                       type = "prob", na.action = na.pass)[,1]

# Test data: predicted classes
lda_pred_class_test = predict(LDA_model_caret, newdata = testing_df, type = "raw",
                                     na.action = na.pass)

# Test data: confusion matrix
# Accuracy: 0.85
confusionMatrix(data = lda_pred_class_test,
                reference = y_test)
```



```{r model tuning setup full data set}
### Using Zak's cleaned_df but I created two partitions, one for without NA, another with NA.

## NA omitted data set
cleaned_df_na_omit <- cleaned_df %>% na.omit()

# Training/testing partition
set.seed(2022)
index_train = createDataPartition(cleaned_df$ten_year_chd, 
                                  p = 0.8,
                                  list = FALSE)

set.seed(2022)
index_train_na_omit = createDataPartition(cleaned_df_na_omit$ten_year_chd, 
                                          p = 0.8,
                                          list = FALSE)

training_df = cleaned_df[index_train, ]
testing_df = cleaned_df[-index_train, ]

training_df_na_omit <- training_df %>%  na.omit()
testing_df_na_omit <- testing_df %>% na.omit()

#Train control with 10-fold cross-validation repeated 5 times
ctrl = trainControl(method = "repeatedcv",
                    repeats = 5,
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE)
```

```{r}
### Neural Network Model
set.seed(2022)
nnet_fit <- train(ten_year_chd~.,
                  data = training_df_na_omit,
                  method = "nnet",
                  tuneGrid =  expand.grid(decay = exp(seq(-1, 2.5, len = 15)),  
                                          size = c(1:5)),
                  metric = "ROC",
                  preProcess = c("center", "scale", "BoxCox"),
                  trControl = ctrl)

#Note that when we don't impute the model, we should use testing data without missing values
nnet_pred_test_probs <- predict(nnet_fit, newdata = testing_df_na_omit, type = "prob")[,1]
```

```{r}
### Neural Network Model with imputation
nnet_fit_impute <- train(preprocess_recipe,
                         data = training_df,
                         method = "nnet",
                         tuneGrid =  expand.grid(decay = exp(seq(-1, 2.5, len = 15)),  
                                                 size = c(1:5)),
                         metric = "ROC",
                         trControl = ctrl)

#Note that when we impute the model, we should use testing data with missing values (NA)
nnet_impute_pred_test_probs <- predict(nnet_fit_impute, newdata = testing_df, type = "prob")[,1]
```

```{r}
### Averaged Neural Network Model
#library(nnet)
#Fitting Model Averaged Neural Network
#Tuning parameters:
#size (#Hidden Units)
#decay (Weight Decay)
#bag (Bagging)

my.grid <- expand.grid(decay = seq(0, 0.001, 0.0001), 
                       size = c(1:7), 
                       bag = TRUE)

# get the maximum number of hidden units
maxSize <- max(my.grid$size)

# compute the maximum number of parameters
# there are M(p+1)+M+1 parameters in total, excluding outcome variable
numWts <- 1*(maxSize * (length(training_df_na_omit) -1 + 1) + maxSize + 1)

set.seed(2022)
avnet_fit <- train(ten_year_chd ~ .,
                   data = training_df_na_omit,
                   method = "avNNet",
                   MaxNWts = numWts, #maximum allowable weights
                   maxit = 1000, 
                   tuneGrid = my.grid, 
                   trace = FALSE, 
                   metric = "ROC",
                   preProcess = c("center", "scale", "BoxCox"),
                   trControl = ctrl)  

avnnet_pred_test_probs <- predict(avnet_fit, newdata = testing_df_na_omit, type = "prob")[,1]
```


```{r}
### Averaged Neural Network Model with imputation

set.seed(2022)
avnet_fit_impute <- train(preprocess_recipe,
                          data = training_df,
                          method = "avNNet",
                          MaxNWts = numWts, #maximum allowable weights
                          maxit = 1000, 
                          tuneGrid = my.grid, 
                          trace = FALSE, 
                          metric = "ROC",
                          trControl = ctrl)

avnnet_impute_pred_test_probs <- predict(avnet_fit_impute, newdata = testing_df, type = "prob")[,1]
```

```{r}
###  Extreme Gradient Boosting Model
#Fitting Extreme Gradient Boosting
#library(xgboost)
#Tuning PARAMETERS:
#nrounds (# Boosting Iterations)
#lambda (L2 Regularization)
#alpha (L1 Regularization)
#eta (Learning Rate)

set.seed(2022)
xgbGrid <- expand.grid(nrounds = c(1:15), 
                       lambda = exp(seq(-1, 2, len = 10)) %>% round(digits = 0),
                       alpha = exp(seq(-1, 2, len = 10)) %>% round(digits = 0),
                       eta = c(0.00001, 0.0001, 0.001))

x_gradient_boost_fit <- train(ten_year_chd ~ .,
                               data = training_df_na_omit, 
                               method = "xgbLinear", 
                               tuneGrid = xgbGrid,
                               metric = "ROC", 
                               preProcess = c("center", "scale", "BoxCox"),
                               trControl = ctrl)

xgb_impute_pred_test_probs <- 
  predict(ex_gradient_boost_fit, newdata = testing_df_na_omit, type = "prob")[,1]
```


```{r}
### MARS Model
set.seed(2022)
mars_fit <- train(ten_year_chd~.,
                   data = training_df_na_omit,
                   method = "earth",
                   tuneGrid = expand.grid(degree = 1:3, 
                                          nprune = 2:35),
                   metric = "ROC",
                   preProcess = c("center", "scale", "BoxCox"),
                   trControl = ctrl)

mars_pred_test_probs <- predict(mars_fit, newdata = testing_df_na_omit, type = "prob")[,1]
```


```{r}
### MARS Model with imputation
set.seed(2022)
mars_fit_impute <- train(preprocess_recipe,
                         data = training_df,
                         method = "earth",
                         tuneGrid = expand.grid(degree = 1:3, 
                                                nprune = 2:35),
                         metric = "ROC",
                         trControl = ctrl)

mars_impute_pred_test_probs <- predict(mars_fit_impute, newdata = testing_df, type = "prob")[,1]
```


```{r}
### GAM Model
GAM_fit <- train(ten_year_chd~.,
                  data = training_df_na_omit,
                  method = "gam",
                  metric = "ROC",
                  family = "binomial",
                  preProcess = c("center", "scale", "BoxCox"),
                  tuneGrid = data.frame(method = "GCV.Cp", select = c(TRUE, FALSE)),
                  trControl = ctrl)

gam_pred_test_probs <- predict(GAM_fit, newdata = testing_df_na_omit, type = "prob")[,1]
```


```{r'}
### GAM Model with imputation
set.seed(2022)

GAM_fit_impute <- train(preprocess_recipe,
                        data = training_df,
                        method = "gam",
                        metric = "ROC",
                        family = "binomial",
                        tuneGrid = data.frame(method = "GCV.Cp", select = c(TRUE, FALSE)),
                        trControl = ctrl)

gam_impute_pred_test_probs <- predict(GAM_fit_impute, newdata = testing_df, type = "prob")[,1]
```

### Optimal Tuning Parameters

```{r}
# Optimal tuning parameters: GLMNet
# Alpha = 0.3, Lambda = 0.0164
logit_next$bestTune

# Plots of optimal tuning parameters for GLMnet
myCol = rainbow(15)
myPar = list(superpose.symbol = list(col = myCol),
             superpose.line = list(col = myCol))

plot(logit_next, par.settings = myPar, xTrans = function(x) log(x))

logit_tuning_graph = ggplot(logit_next, highlight = T) + 
  scale_x_continuous(trans = "log") + 
  labs(title = "Penalized Logistic Regression",
       x = "Lambda",
       y = "AUC")

# Optimal tuning parameters RF Recipe: 1 randomly selected predictor, min node size = 10
# Note: try tuning parameters > 10 min node size in grid?
ggplot(rf_fit, highlight = TRUE)

# Optimal tuning parameters RF Caret: 1 randomly selected predictor, min node size = 2
ggplot(rf_caret, highlight = TRUE)

# Optimal tuning parameters Boost Recipe: max tree depth = 1, 5000 boosting iterations, shrinkage = 0.002
# Try more boosting iterations and higher shrinkage?
ggplot(boost_fit, highlight = TRUE)

# Optimal tuning parameters Boost Caret
# Max tree depth = 1, 5000 boosting iterations, shrinkage = 0.002
# Try more boosting iterations and higher shrinkage?
ggplot(boost_caret, highlight = TRUE)

# Optimal tuning parameter CART
ggplot(cart_fit, highlight = TRUE)

# Optimal tuning parameter CIT
# Re-run with changed mincriterion (tuning parameter)
ggplot(cit_fit, highlight = TRUE)

# Optimal tuning parameters SVM linear
plot(svm_linear_fit, highlight = TRUE, xTrans = log)

# Optimal tuning parameters SVM radial
plot(svm_radial_fit_impute, transform.y = log,
     transform.x = log)

# Optimal tuning parameters Neural Network
ggplot(nnet_fit, highlight = T)

# Optimal tuning parameters Averaged Neural Network
ggplot(avnet_fit, highlight = T)

# Optimal tuning parameters Extreme Gradient Boosting Model
ggplot(x_gradient_boost_fit, highlight = T)

# Optimal tuning parameters MARS
ggplot(mars_fit, highlight = T)

# GAM tuning parameter plot looks like an empty plot (professor also doesn't include in her code)
```

### Variable Importance

```{r}
# Variable importance: GLMNet
# Most important variables: age, sys_bp, sexfemale, cigs_per_day
logit_vip_graph = vip(logit_next, num_features = 20, method = "model")

# Variable importance RF recipe: permutation / gini
# sys_bp, dia_bp, glucose, prevalent_hyp
rf_recipe_var_imp_permutation = ranger(ten_year_chd ~ ., 
                           training_df[complete.cases(training_df),],
                           mtry = rf_fit$bestTune[[1]],
                           splitrule = "gini",
                           min.node.size = rf_fit$bestTune[[3]],
                           importance = "permutation",
                           scale.permutation.importance = TRUE)

barplot(sort(ranger::importance(rf_recipe_var_imp_permutation), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan", "blue"))(12))

# Variable importance RF recipe: impurity / gini
# sys_bp, age, dia_bp, glucose
rf_recipe_var_imp_impurity = ranger(ten_year_chd ~ ., 
                           training_df[complete.cases(training_df),],
                           mtry = rf_fit$bestTune[[1]],
                           splitrule = "gini",
                           min.node.size = rf_fit$bestTune[[3]],
                           importance = "impurity",
                           scale.permutation.importance = TRUE)

barplot(sort(ranger::importance(rf_recipe_var_imp_impurity), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan", "blue"))(12))

# Variable importance RF caret: permutation / gini
# sys_bp, dia_bp, prevalent_hyp, glucose
rf_caret_var_imp_permutation = ranger(ten_year_chd ~ ., 
                           training_df[complete.cases(training_df),],
                           mtry = rf_caret$bestTune[[1]],
                           splitrule = "gini",
                           min.node.size = rf_caret$bestTune[[3]],
                           importance = "permutation",
                           scale.permutation.importance = TRUE)

barplot(sort(ranger::importance(rf_caret_var_imp_permutation), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan", "blue"))(12))

# Variable importance RF caret: impurity / gini
# sys_bp, age, dia_bp, glucose
rf_caret_var_imp_impurity = ranger(ten_year_chd ~ ., 
                           training_df[complete.cases(training_df),],
                           mtry = rf_caret$bestTune[[1]],
                           splitrule = "gini",
                           min.node.size = rf_caret$bestTune[[3]],
                           importance = "impurity",
                           scale.permutation.importance = TRUE)

barplot(sort(ranger::importance(rf_caret_var_imp_impurity), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan", "blue"))(12))

# Variable importance Boost Recipe
# age, sys_bp, glucose, cigs_per_day
summary(boost_fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)

# Variable importance Boost Caret
# age, sys_bp, glucose, cigs_per_day
summary(boost_caret$finalModel, las = 2, cBars = 19, cex.names = 0.6)

# Final model CART
rpart.plot(cart_fit$finalModel)

# Final model CIT
plot(cit_fit$finalModel)

# Variable importance Neural Network
V <- varImp(nnet_fit$finalModel)

ggplot(V, aes(x = reorder(rownames(V),Overall), y = Overall)) +
  geom_point(color = "blue", size = 2, alpha = 0.6) +
  geom_segment(aes(x = rownames(V), xend = rownames(V), y = 0, yend = Overall), 
              color = 'skyblue') +
  xlab('Variable') +
  ylab('Overall Importance') +
  theme_light() +
  coord_flip() 

# Neural Network model plot
plotnet(nnet_fit)

# Variable importance Averaged Neural Network
names <- avnet_fit$coefnames
V2 = varImp(avnet_fit$finalModel)
rownames(V2) <- c(names[1], names[10], names[11], names[12], names[13], names[14],
                 names[15], names[16], names[17], names[2], names[3], names[4],
                 names[5], names[6], names[7], names[8], names[9])

ggplot(V2, aes(x = reorder(rownames(V2),Overall), y = Overall)) +
  geom_point(color = "blue", size = 2, alpha = 0.6) +
  geom_segment(aes(x = rownames(V2), xend = rownames(V2), y = 0, yend = Overall), 
              color = 'skyblue') +
  xlab('Variable') +
  ylab('Overall Importance') +
  theme_light() +
  coord_flip() 

# Variable importance Extreme Gradient Boosting Model
importance_df <- xgb.importance(model = x_gradient_boost_fit$finalModel)
xgb.ggplot.importance(importance_df, top_n = 15, measure = "Gain")

# Variable importance MARS
V3 <- varImp(mars_fit$finalModel)

ggplot(V3, aes(x = reorder(rownames(V3),Overall), y = Overall)) +
  geom_point(color = "blue", size = 2, alpha = 0.6) +
  geom_segment(aes(x = rownames(V3), xend = rownames(V3), y = 0, yend = Overall), 
              color = 'skyblue') +
  xlab('Variable') +
  ylab('Overall Importance') +
  theme_light() +
  coord_flip()

# Variable importance GAM
V4 <- varImp(GAM_fit$finalModel)

ggplot(V4, aes(x = reorder(rownames(V4),Overall), y = Overall)) +
  geom_point(color = "blue", size = 2, alpha = 0.6) +
  geom_segment(aes(x = rownames(V4), xend = rownames(V4), y = 0, yend = Overall), 
              color = 'skyblue') +
  xlab('Variable') +
  ylab('Overall Importance') +
  theme_light() +
  coord_flip()
```


### Resampling Results and Model Selection

```{r resampling train results, include = TRUE}
# Results from resampling on training data 
resamp = resamples(list(random_forest_recipes_impute = rf_fit, 
                        adaboost_recipes_impute = boost_fit,
                        glmnet = logit_next,
                        random_forest_caret_impute = rf_caret,
                        boost_caret_impute = boost_caret,
                        cart_tree = cart_fit,
                        cit_tree = cit_fit,
                        svm_linear = svm_linear_fit,
                        svm_radial = svm_radial_fit_impute,
                        lda = LDA_model_caret,
                        Neural_Network = nnet_fit,
                        Neural_Network_Impute = nnet_fit_impute,
                        Aveeraged_Neural_Network = avnet_fit,
                        Aveeraged_Neural_Network_Impute = avnet_fit_impute,
                        Extreme_Gradient_Boosting = ex_gradient_boost_fit,
                        MARS = mars_fit,
                        MARS_Impute = mars_fit_impute,
                        GAM = GAM_fit,
                        GAM_Impute = GAM_fit_impute))

# Median AUC is highest for glmnet (0.727), and boost_caret_impute (0.722)                   
summary(resamp)

bwplot(resamp, layout = c(3, 1))
```

### Model Application to Test Data

```{r auc and roc testing, include = TRUE}
# ROC curves for fitted models applied to testing data
# AUC is highest for glmnet and boost_caret_impute (0.74 for both)

roc_rf_recipes_impute = roc(y_test, rf_pred_test_probs)
roc_boost_impute = roc(y_test, boost_pred_test_probs)
roc_glmnet = roc(y_test, glmnet_pred_test_probs)
roc_rf_caret_impute = roc(y_test, rf_caret_pred_test_probs)
roc_boost_caret_impute = roc(y_test, boost_caret_pred_test_probs)
roc_cart_impute = roc(y_test, cart_pred_prob_test)
roc_cit_impute = roc(y_test, cit_pred_prob_test)
roc_svm_linear_impute = roc(y_test, svm_linear_pred_prob_test)
roc_svm_radial_impute = roc(y_test, svm_radial_pred_prob_test)
roc_lda = roc(y_test, lda_pred_prob_test)

roc_nnet <- roc(testing_df_na_omit$ten_year_chd, glm.pred)
roc_nnet_impute <- roc(testing_df$ten_year_chd, glm.pred)
roc_avnnet <- roc(testing_df_na_omit$ten_year_chd, glm.pred)
roc_avnnet_impute <- roc(testing_df$ten_year_chd, glm.pred)
xgb_roc <- roc(testing_df_na_omit$ten_year_chd, glm.pred)
roc_mars <- roc(testing_df_na_omit$ten_year_chd, glm.pred)
roc_mars_impute <- roc(testing_df$ten_year_chd, glm.pred)
roc_gam <- roc(testing_df_na_omit$ten_year_chd, glm.pred)
roc_gam_impute <- roc(testing_df$ten_year_chd, glm.pred)

plot(roc_rf_recipes_impute, col = 1)
plot(roc_boost_impute, add = TRUE, col = 2)
plot(roc_glmnet, add = TRUE, col = 3)
plot(roc_rf_caret_impute, add = TRUE, col = 4)
plot(roc_boost_caret_impute, add = TRUE, col = 5)
plot(roc_cart_impute, add = TRUE, col = 6)
plot(roc_cit_impute, add = TRUE, col = 7)
plot(roc_svm_linear_impute, add = TRUE, col = 8)
plot(roc_svm_radial_impute, add = TRUE, col = 9)
plot(roc_lda, add = TRUE, col = 10)
plot(roc_nnet, add = TRUE, col = 11)
plot(roc_nnet_impute, add = TRUE, col = 12)
plot(roc_avnnet, add = TRUE, col = 13)
plot(roc_avnnet_impute, add = TRUE, col = 14)
plot(xgb_roc, add = TRUE, col = 15)
plot(roc.mars, add = TRUE, col = 16)
plot(roc_mars_impute, add = TRUE, col = 17)
plot(roc_gam, add = TRUE, col = 18)
plot(roc_gam_impute, add = TRUE, col = 19)

auc <- sort(c(roc_rf_recipes_impute$auc[1], roc_boost_impute$auc[1], roc_glmnet$auc[1], 
              roc_rf_caret_impute$auc[1], roc_boost_caret_impute$auc[1], roc_cart_impute$auc[1],
              roc_cit_impute$auc[1], roc_svm_linear_impute$auc[1], roc_svm_radial_impute$auc[1],
              roc_lda$auc[1], roc.nnet$auc[1], roc.nnet.impute$auc[1], roc.avnnet$auc[1], 
              roc.avnnet.impute$auc[1], egb.roc$auc[1], roc.mars$auc[1],
              roc.mars.impute$auc[1], roc.gam$auc[1], roc.gam.impute$auc[1]), decreasing = TRUE)

model_names = c("Random Forest w/ Recipes", "Adaboost w/ Recipes", "GLMNet w/ Caret", 
                "Random Forest w/ Caret", "Boost w/ Caret", "CART w/ Recipes", "CIT w/ Recipes", 
                "Linear SVM w/ Caret", "Radial SVM w/ Caret", "LDA w/ Caret", 
                "Neural Network","Neural Network Impute","Averaged Neural Network", 
                "Averaged Neural Network Impute", "Extreme Gradient Boosting",
                "MARS", "MARS Impute", "GAM", "GAM Impute")

legend("bottomright", legend = paste0(model_names, ": ", round(auc, 3)),
       col = 1:19, lwd = 2)
```

## Conclusion

TO DO

## Still In Progress

```{r SVM radial}
# Trying SVM with radial classifier for fun
# set.seed(2022)
# 
# svm_grid = expand.grid(C = exp(seq(-2, 3, len = 50)),
#                        sigma = exp(seq(-3, 0, len = 50)))
# 
# svm_fit_impute_classes = train(preprocess_recipe,
#                       data = training_df,
#                       method = "svmRadialSigma",
#                       tuneGrid = svm_grid,
#                       trControl = ctrl)
# 
# # I know we're told not to do this, but including Platt's probabilistic outputs here just to see...
# svm_fit_impute_probs = train(preprocess_recipe,
#                             data = training_df,
#                             method = "svmRadialSigma",
#                             tuneGrid = svm_grid,
#                             trControl = ctrl,
#                             prob.model = TRUE)
```

```{r SVM linear recipe}
# SVM with linear kernel, imputation from recipe
# Doesn't work; will try impute from train function instead
# set.seed(2022)
# 
# svm_linear_fit = train(preprocess_recipe,
#                        data = training_df,
#                        method = "svmLinear",
#                        tuneGrid = data.frame(C = exp(seq(-5, 2, len = 50))),
#                        trControl = ctrl
#                        )
# 
# plot(svm_linear_fit, highlight = TRUE, xTrans = log)
```

```{r GLMNet with recipe}
# # Penalized logistic regression
# # Doesn't work
# # https://stackoverflow.com/questions/48179423/error-error-in-lognetx-is-sparse-ix-jx-y-weights-offset-alpha-nobs/48230658#48230658
# set.seed(2022)
# 
# glm_grid = expand.grid(alpha = seq(0, 1, length = 11),
#                        lambda = exp(seq(-8, -3, length = 19)))
# 
# logit_glm = train(preprocess_recipe,
#                   data = training_df,
#                   method = "glmnet",
#                   tuneGrid = glm_grid,
#                   metric = "ROC",
#                   trControl = ctrl,
#                   family = "binomial")
```
