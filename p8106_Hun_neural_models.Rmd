---
title: "p8106_Hun_Models"
author: "Hun"
date: '2022-05-05'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cahce = TRUE, echo = FALSE, message = FALSE, warning = FALSE)
```

```{r libraries and setup, include = FALSE, echo = FALSE}
# Includes excess libraries (for now)
library(tidyverse)
library(ggplot2)
library(skimr)
library(visdat)
library(reshape2)
library(caret)
library(vip)
library(mgcv)
library(klaR)
library(pROC)
library(ggridges)
library(corrplot)
library(AppliedPredictiveModeling)
library(patchwork)
library(Hmisc)
library(reshape)
library(recipes)
library(glmnet)
library(rpart.plot)
library(MASS)
library(nnet)
library(RSNNS)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE,
  fig.height = 6,
  fig.width = 8
  )

theme_set(theme_minimal() + theme(
  legend.position = "bottom",
  plot.title = element_text(hjust = 0.5)
))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis",
  digits = 3)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Introduction

```{r importing and cleaning data}
# Read in all data
# source: https://raw.githubusercontent.com/TarekDib03/Analytics/master/Week3%20-%20Logistic%20Regression/Data/framingham.csv
all_df = read_csv("FHS.csv")

# Factor labels for categorical variables and other recoding
cleaned_df = all_df %>% 
  mutate(male = factor(male),
         current_smoker = factor(current_smoker),
         bp_meds = factor(bp_meds),
         prevalent_stroke = factor(prevalent_stroke),
         prevalent_hyp = factor(prevalent_hyp),
         diabetes = factor(diabetes),
         ten_year_chd = factor(ten_year_chd))  %>%
  mutate(ten_year_chd = ifelse(ten_year_chd == "1", "CHD_present","CHD_absent") %>%
           fct_relevel("CHD_present", "CHD_absent")) %>%
  dplyr::rename(sex = male) %>%
  mutate(sex = ifelse(sex == "1", "male","female") %>%
           fct_relevel("male", "female")) %>% 
  mutate(
    education = case_when(
      education == "1" ~ "some_HS",
      education == "2" ~ "HS_grad",
      education == "3" ~ "some_college",
      education == "4" ~ "college_grad"
    ),
    current_smoker = recode(
      current_smoker,
      "1" = "yes",
      "0" = "no"
    ),
    bp_meds = recode(
      bp_meds,
      "1" = "yes",
      "0" = "no"
    ),
    prevalent_stroke = recode(
      prevalent_stroke,
      "1" = "yes",
      "0" = "no"
    ),
    prevalent_hyp = recode(
      prevalent_hyp,
      "1" = "yes",
      "0" = "no"
    ),
    diabetes = recode(
      diabetes,
      "1" = "yes",
      "0" = "no"
    ),
    education = factor(education, levels = c("some_HS", "HS_grad", "some_college", "college_grad"))
  )
```


```{r}
cleaned_df_na_omit <- cleaned_df %>% na.omit()
```

## Models

```{r model tuning setup full data set}
set.seed(2022)

# Training/testing partition
index_train = createDataPartition(cleaned_df$ten_year_chd, 
                                  p = 0.8,
                                  list = FALSE)

index_train_na_omit = createDataPartition(cleaned_df_na_omit$ten_year_chd, 
                                          p = 0.8,
                                          list = FALSE)

training_df = cleaned_df[index_train, ]
testing_df = cleaned_df[-index_train, ]

training_df_na_omit <- training_df %>%  na.omit()
testing_df_na_omit <- testing_df %>% na.omit()

# Train control with 5-fold cross-validation
ctrl = trainControl(method = "cv",
                    number = 5,
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE,
                    verboseIter = TRUE)
```

```{r preprocessing with recipe}
# Preprocessing and feature engineering with recipe (including imputation)
# Note: assuming data is MAR

# recipe of preprocessing steps
preprocess_recipe = recipe(ten_year_chd ~ ., data = training_df) %>%
  step_impute_knn(all_predictors(), neighbors = 5) %>%  # KNN imputation based on 5 nearest neighbors
  step_BoxCox(all_numeric_predictors()) %>% # transform predictors
  step_center(all_numeric_predictors()) %>% # center and scale numeric predictors
  step_scale(all_numeric_predictors())
```

```{r}
#library(nnet)
#Fitting Model Averaged Neural Network
#Tuning parameters:
#size (#Hidden Units)
#decay (Weight Decay)
#bag (Bagging)

my.grid <- expand.grid(decay = c(0, 0.0001, 0.001, 0.01), 
                       size = c(1:7), 
                       bag = FALSE)

# get the maximum number of hidden units
maxSize <- max(my.grid$size)
# compute the maximum number of parameters
# there are M(p+1)+M+1 parameters in total, excluding outcome variable
numWts <- 1*(maxSize * (length(training_df_na_omit) -1 + 1) + maxSize + 1)

set.seed(2022)
avnet_fit <- train(ten_year_chd ~ .,
                   data = training_df_na_omit,
                   method = "avNNet",
                   MaxNWts = numWts, #maximum allowable weights
                   maxit = 1000, 
                   tuneGrid = my.grid, 
                   trace = FALSE, 
                   metric = "ROC",
                   preProcess = c("center", "scale", "BoxCox"),
                   trControl = ctrl)  
```

```{r}
plot(avnet_fit)
avnet_fit$bestTune
```

```{r}
avnet_fit$resample
```


```{r}
#with imputation
set.seed(2022)
avnet_fit_impute <- train(preprocess_recipe,
                          data = training_df,
                          method = "avNNet",
                          MaxNWts = numWts, #maximum allowable weights
                          maxit = 1000, 
                          tuneGrid = my.grid, 
                          trace = FALSE, 
                          metric = "ROC",
                          trControl = ctrl)  
```

```{r}
plot(avnet_fit_impute)
avnet_fit_impute$bestTune
```
```{r}
avnet_fit_impute$resample
```



```{r}
#library(RSNNS)
#Fitting Multi-layer perception Neural Network
#Tuning parameters:
#layer1 (#Hidden Units layer1)
#layer2 (#Hidden Units layer2)
#layer3 (#Hidden Units layer3)
#decay (Weight Decay)

grid <- expand.grid(decay = c(0, 0.0001, 0.001), layer1 = 1:3, layer2 = 1:3, layer3 = 1:3)

set.seed(2022)
mlnet_fit <- train(ten_year_chd ~ .,
                   data = training_df_na_omit,
                   method = "mlpWeightDecayML",
                   tuneGrid = grid,
                   metric = "ROC",
                   preProcess = c("center", "scale", "BoxCox"),
                   trControl = ctrl)
```

```{r}
plot(mlnet_fit)
mlnet_fit$bestTune
```

```{r}
mlnet_fit$resample
```
```{r}
#caret attaches 2 packages which are "RSNNS" and "Rcpp
#for some reason they don't work with imputation

#With Imputation
#set.seed(2022)

#ctrl_impute <- trainControl(method = "cv",
#                            number = 5,
#                            summaryFunction = twoClassSummary,
#                            classProbs = TRUE,
#                            preProcOptions = list(k = 5),
#                            verboseIter = TRUE)


#mlnet_fit_impute <- train(preprocess_recipe,
#                          data = training_df,
#                          method = "mlpWeightDecayML",
#                          tuneGrid = grid,
#                          metric = "ROC",
#                          trControl = ctrl)
```


